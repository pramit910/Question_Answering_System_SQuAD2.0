{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6n6_KDPHCzfj"
      },
      "source": [
        "# **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3EVM8p56tpK",
        "outputId": "50e15e99-5662-4ecf-ceae-6e1ace3b79c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "UvDcnyzhAwS2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from pprint import pprint\n",
        "import random\n",
        "from transformers import AutoTokenizer\n",
        "import os, json\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AztbimcDWv0o"
      },
      "source": [
        "# **Question Answering using ALBERT Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "RWH-zfRELwre"
      },
      "outputs": [],
      "source": [
        "# Function to read input from the specified path\n",
        "\n",
        "def read_input(file: str) -> tuple:    \n",
        "    location = os.path.join(os.getcwd(), file)\n",
        "    with open(location, \"rb\") as json_input:\n",
        "        dictionary = json.load(json_input)\n",
        "    contexts, questions, answers = list(), list(), list()\n",
        "    \n",
        "# Extracting the contexts, questions and answers from the SQUAD 2.0 dataset\n",
        "\n",
        "    for sample in dictionary['data']:\n",
        "        for passage in sample['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                \n",
        "# Answers and Plausible answers separation\n",
        "\n",
        "                access = \"plausible_answers\" if \"plausible_answers\" in qa.keys() else 'answers'\n",
        "                for answer in qa[access]:\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    answers.append(answer)\n",
        "    \n",
        "    return contexts, questions, answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "Kr4ITGV3L20N"
      },
      "outputs": [],
      "source": [
        "train_contexts, train_questions, train_answers = read_input('train-v2.0.json')\n",
        "valid_contexts, valid_questions, valid_answers = read_input('dev-v2.0.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_ysQvcKL4LY",
        "outputId": "9a912d72-40dc-4cca-baf1-4b95236de2e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q:   What style came before Modernism?\n",
            "\n",
            "Context:\n",
            "\n",
            "('Architects such as Mies van der Rohe, Philip Johnson and Marcel Breuer '\n",
            " 'worked to create beauty based on the inherent qualities of building '\n",
            " 'materials and modern construction techniques, trading traditional historic '\n",
            " 'forms for simplified geometric forms, celebrating the new means and methods '\n",
            " 'made possible by the Industrial Revolution, including steel-frame '\n",
            " 'construction, which gave birth to high-rise superstructures. By mid-century, '\n",
            " 'Modernism had morphed into the International Style, an aesthetic epitomized '\n",
            " \"in many ways by the Twin Towers of New York's World Trade Center designed by \"\n",
            " 'Minoru Yamasaki.')\n",
            "\n",
            "Answer:[{'text': 'International', 'answer_start': 464}]\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Q:  What label avoided this scene?\n",
            "\n",
            "Context:\n",
            "\n",
            "('The innovative production techniques devised by post-punk producers such as '\n",
            " 'Martin Hannett and Dennis Bovell during this period would become an '\n",
            " 'important element of the emerging music, with studio experimentation taking '\n",
            " 'a central role. A variety of groups that predated punk, such as Cabaret '\n",
            " 'Voltaire and Throbbing Gristle, experimented with crude production '\n",
            " 'techniques and electronic instruments in tandem with performance art methods '\n",
            " 'and influence from transgressive literature, ultimately helping to pioneer '\n",
            " \"industrial music. Throbbing Gristle's independent label Industrial Records \"\n",
            " 'would become a hub for this scene and provide it with its namesake.')\n",
            "\n",
            "Answer:[{'text': 'Industrial Records', 'answer_start': 567}]\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Q:  When did the Castro culture flourish?\n",
            "\n",
            "Context:\n",
            "\n",
            "(\"The Castro culture ('Culture of the Castles') developed during the Iron Age, \"\n",
            " 'and flourished during the second half of the first millennium BC. It is '\n",
            " 'usually considered a local evolution of the Atlantic Bronze Age, with later '\n",
            " 'developments and influences and overlapping into the Roman era. '\n",
            " 'Geographically, it corresponds to the people Roman called Gallaeci, which '\n",
            " 'were composed by a large series of nations or tribes, among them the '\n",
            " 'Artabri, Bracari, Limici, Celtici, Albiones and Lemavi. They were capable '\n",
            " 'fighters: Strabo described them as the most difficult foes the Romans '\n",
            " 'encountered in conquering Lusitania, while Appian mentions their warlike '\n",
            " 'spirit, noting that the women bore their weapons side by side with their '\n",
            " 'men, frequently preferring death to captivity. According to Pomponius Mela '\n",
            " 'all the inhabitants of the coastal areas were Celtic people.')\n",
            "\n",
            "Answer:[{'text': 'second half of the first millennium BC.', 'answer_start': 103}]\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Q:  Who are 16 of 20 of the middle class electorates held by?\n",
            "\n",
            "Context:\n",
            "\n",
            "('Throughout their history, the Liberals have been in electoral terms largely '\n",
            " \"the party of the middle class (whom Menzies, in the era of the party's \"\n",
            " 'formation called \"The forgotten people\"), though such class-based voting '\n",
            " 'patterns are no longer as clear as they once were. In the 1970s a left-wing '\n",
            " 'middle class emerged that no longer voted Liberal.[citation needed] One '\n",
            " 'effect of this was the success of a breakaway party, the Australian '\n",
            " 'Democrats, founded in 1977 by former Liberal minister Don Chipp and members '\n",
            " 'of minor liberal parties; other members of the left-leaning section of the '\n",
            " 'middle-class became Labor supporters.[citation needed] On the other hand, '\n",
            " 'the Liberals have done increasingly well in recent years among socially '\n",
            " 'conservative working-class voters.[citation needed]However the Liberal '\n",
            " \"Party's key support base remains the upper-middle classes; 16 of the 20 \"\n",
            " 'richest federal electorates are held by the Liberals, most of which are safe '\n",
            " 'seats. In country areas they either compete with or have a truce with the '\n",
            " 'Nationals, depending on various factors.')\n",
            "\n",
            "Answer:[{'text': 'Liberals', 'answer_start': 920}]\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Q:  Scientists seeking a master's degree might be interested in which other degree besides Science Communication from the Science Communication Unit?\n",
            "\n",
            "Context:\n",
            "\n",
            "('The Centre For Co-Curricular Studies provides elective subjects and language '\n",
            " 'courses outside the field of science for students in the other faculties and '\n",
            " 'departments. Students are encouraged to take these classes either for credit '\n",
            " 'or in their own time, and in some departments this is mandatory. Courses '\n",
            " 'exist in a wide range of topics including philosophy, ethics in science and '\n",
            " 'technology, history, modern literature and drama, art in the 20th century, '\n",
            " 'film studies. Language courses are available in French, German, Japanese, '\n",
            " 'Italian, Russian, Spanish, Arabic and Mandarin Chinese. The Centre For '\n",
            " 'Co-Curricular Studies is home to the Science Communication Unit which offers '\n",
            " \"master's degrees in Science Communication and Science Media Production for \"\n",
            " 'science graduates.')\n",
            "\n",
            "Answer:[{'text': 'Science Media Production', 'answer_start': 723}]\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Randomly printing 5 contexts, questions and answers from the dataset\n",
        "\n",
        "random.seed(89)\n",
        "\n",
        "ind = random.sample(range(0, len(train_contexts)), 5)\n",
        "for index in ind:\n",
        "    print(f'Q:  {train_questions[index]}\\n')\n",
        "    print(\"Context:\\n\")\n",
        "    pprint(train_contexts[index])\n",
        "    print(f\"\\nAnswer:[{train_answers[index]}]\\n\")\n",
        "    print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "rd4BySotL6vv"
      },
      "outputs": [],
      "source": [
        "# Since the dataset only has start_index, this function calculates the End index and appends it to the train_answers and valid_answers variables\n",
        "\n",
        "def answer_end(answers: list, contexts: list) -> list:\n",
        "    _answers = answers.copy()\n",
        "    for answer, context in zip(_answers, contexts):\n",
        "        answer_bound = answer['text']\n",
        "        start_idx = answer['answer_start']\n",
        "        answer['answer_end'] = start_idx + len(answer_bound)\n",
        "    return _answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "VGWhQ8nfL8ND"
      },
      "outputs": [],
      "source": [
        "train_answers = answer_end(train_answers, train_contexts)\n",
        "valid_answers = answer_end(valid_answers, valid_contexts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "zs7l5sCAL9v2"
      },
      "outputs": [],
      "source": [
        "# Intializing the autotokenizer library for the model 'distilbert-base-uncased'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('albert-base-v2', use_fast=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "thiCA2RsL9ub"
      },
      "outputs": [],
      "source": [
        "# Encoding the dataset contexts, questions and answers\n",
        "\n",
        "def encode_data(contexts: list, questions: list, answers: list) -> dict:\n",
        "\n",
        "# Tokenizing the input and extracting the encodings \n",
        "\n",
        "    encodings = tokenizer(contexts, questions, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    start_positions, end_positions = list(), list()\n",
        "\n",
        "    for index in range(len(answers)):\n",
        "        start_value = encodings.char_to_token(index, answers[index]['answer_start'])\n",
        "        end_value   = encodings.char_to_token(index, answers[index]['answer_end'])\n",
        "\n",
        "# if start position is None, answer is truncated\n",
        "\n",
        "        if start_value is None:\n",
        "            start_value = tokenizer.model_max_length\n",
        "        \n",
        "# If end position cannot be found, shift position until found\n",
        "\n",
        "        shift = 1\n",
        "        while end_value is None:\n",
        "            end_value = encodings.char_to_token(index, answers[index]['answer_end'] - shift)\n",
        "            shift += 1\n",
        "\n",
        "        start_positions.append(start_value)\n",
        "        end_positions.append(end_value)\n",
        "\n",
        "    encodings.update({\n",
        "        'start_positions': start_positions, 'end_positions': end_positions\n",
        "    })\n",
        "    return encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "F0IiiIe2MA_b"
      },
      "outputs": [],
      "source": [
        "# Training the entire datset was taking more than 6 days to train. So we have reduced the input dataset size to 10000 contexts and evaluation size to 500 questions for simplicity\n",
        "\n",
        "train_encodings = encode_data(train_contexts[0:5000], train_questions[0:5000], train_answers[0:5000])\n",
        "valid_encodings = encode_data(valid_contexts[0:500], valid_questions[0:500], valid_answers[0:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "1NrKtbK0MCTH"
      },
      "outputs": [],
      "source": [
        "# Deleting the old variables containing non-encoded data\n",
        "\n",
        "del train_contexts, train_questions, train_answers\n",
        "del valid_contexts, valid_questions, valid_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "-gbsVrlMMDqk"
      },
      "outputs": [],
      "source": [
        "# Importing the pytorch for model bulding and intitializing dataset\n",
        "\n",
        "import torch\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings: dict) -> None:\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, index: int) -> dict:\n",
        "        return {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "zdCY-WJiMFwK"
      },
      "outputs": [],
      "source": [
        "train_ds = SquadDataset(train_encodings)\n",
        "valid_ds = SquadDataset(valid_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqVgNioxMHma",
        "outputId": "ef8e8267-9e26-4eeb-cff1-ffb7c0844a8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']\n",
            "- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AlbertForQuestionAnswering(\n",
              "  (albert): AlbertModel(\n",
              "    (embeddings): AlbertEmbeddings(\n",
              "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 128)\n",
              "      (token_type_embeddings): Embedding(2, 128)\n",
              "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0, inplace=False)\n",
              "    )\n",
              "    (encoder): AlbertTransformer(\n",
              "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
              "      (albert_layer_groups): ModuleList(\n",
              "        (0): AlbertLayerGroup(\n",
              "          (albert_layers): ModuleList(\n",
              "            (0): AlbertLayer(\n",
              "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (attention): AlbertAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (attention_dropout): Dropout(p=0, inplace=False)\n",
              "                (output_dropout): Dropout(p=0, inplace=False)\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              )\n",
              "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (activation): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForQuestionAnswering\n",
        "model = AutoModelForQuestionAnswering.from_pretrained('albert-base-v2')\n",
        "# setup GPU/CPU\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# move model over to detected device\n",
        "model.to(device)\n",
        "# activate training mode of model\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "Kud3G6wHMJ4u"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This cell is adopted from `https://github.com/michaelrzhang/lookahead/blob/master/lookahead_pytorch.py`, which is the\n",
        "source code of `Lookahead Optimizer: k steps forward, 1 step back` paper (https://arxiv.org/abs/1907.08610).\n",
        "\"\"\"\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "\n",
        "class Lookahead(Optimizer):\n",
        "    r\"\"\"PyTorch implementation of the lookahead wrapper.\n",
        "    Lookahead Optimizer: https://arxiv.org/abs/1907.08610\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, la_steps=5, la_alpha=0.8, pullback_momentum=\"none\"):\n",
        "        \"\"\"optimizer: inner optimizer\n",
        "        la_steps (int): number of lookahead steps\n",
        "        la_alpha (float): linear interpolation factor. 1.0 recovers the inner optimizer.\n",
        "        pullback_momentum (str): change to inner optimizer momentum on interpolation update\n",
        "        \"\"\"\n",
        "        self.optimizer = optimizer\n",
        "        self._la_step = 0  # counter for inner optimizer\n",
        "        self.la_alpha = la_alpha\n",
        "        self._total_la_steps = la_steps\n",
        "        pullback_momentum = pullback_momentum.lower()\n",
        "        assert pullback_momentum in [\"reset\", \"pullback\", \"none\"]\n",
        "        self.pullback_momentum = pullback_momentum\n",
        "\n",
        "        self.state = defaultdict(dict)\n",
        "\n",
        "        # Cache the current optimizer parameters\n",
        "        for group in optimizer.param_groups:\n",
        "            for p in group['params']:\n",
        "                param_state = self.state[p]\n",
        "                param_state['cached_params'] = torch.zeros_like(p.data)\n",
        "                param_state['cached_params'].copy_(p.data)\n",
        "                if self.pullback_momentum == \"pullback\":\n",
        "                    param_state['cached_mom'] = torch.zeros_like(p.data)\n",
        "\n",
        "    def __getstate__(self):\n",
        "        return {\n",
        "            'state': self.state,\n",
        "            'optimizer': self.optimizer,\n",
        "            'la_alpha': self.la_alpha,\n",
        "            '_la_step': self._la_step,\n",
        "            '_total_la_steps': self._total_la_steps,\n",
        "            'pullback_momentum': self.pullback_momentum\n",
        "        }\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def get_la_step(self):\n",
        "        return self._la_step\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.optimizer.state_dict()\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.optimizer.load_state_dict(state_dict)\n",
        "\n",
        "    def _backup_and_load_cache(self):\n",
        "        \"\"\"Useful for performing evaluation on the slow weights (which typically generalize better)\n",
        "        \"\"\"\n",
        "        for group in self.optimizer.param_groups:\n",
        "            for p in group['params']:\n",
        "                param_state = self.state[p]\n",
        "                param_state['backup_params'] = torch.zeros_like(p.data)\n",
        "                param_state['backup_params'].copy_(p.data)\n",
        "                p.data.copy_(param_state['cached_params'])\n",
        "\n",
        "    def _clear_and_load_backup(self):\n",
        "        for group in self.optimizer.param_groups:\n",
        "            for p in group['params']:\n",
        "                param_state = self.state[p]\n",
        "                p.data.copy_(param_state['backup_params'])\n",
        "                del param_state['backup_params']\n",
        "\n",
        "    @property\n",
        "    def param_groups(self):\n",
        "        return self.optimizer.param_groups\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single Lookahead optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = self.optimizer.step(closure)\n",
        "        self._la_step += 1\n",
        "\n",
        "        if self._la_step >= self._total_la_steps:\n",
        "            self._la_step = 0\n",
        "            # Lookahead and cache the current optimizer parameters\n",
        "            for group in self.optimizer.param_groups:\n",
        "                for p in group['params']:\n",
        "                    param_state = self.state[p]\n",
        "                    p.data.mul_(self.la_alpha).add_(param_state['cached_params'], alpha=1.0 - self.la_alpha)  # crucial line\n",
        "                    param_state['cached_params'].copy_(p.data)\n",
        "                    if self.pullback_momentum == \"pullback\":\n",
        "                        internal_momentum = self.optimizer.state[p][\"momentum_buffer\"]\n",
        "                        self.optimizer.state[p][\"momentum_buffer\"] = internal_momentum.mul_(self.la_alpha).add_(\n",
        "                            1.0 - self.la_alpha, param_state[\"cached_mom\"])\n",
        "                        param_state[\"cached_mom\"] = self.optimizer.state[p][\"momentum_buffer\"]\n",
        "                    elif self.pullback_momentum == \"reset\":\n",
        "                        self.optimizer.state[p][\"momentum_buffer\"] = torch.zeros_like(p.data)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dcjuoU2DMwDs"
      },
      "source": [
        "**Model training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "psX_aqIbMLqf"
      },
      "outputs": [],
      "source": [
        "# Initialize adam optimizer with weight decay to minimize overfit\n",
        "\n",
        "from transformers import AdamW\n",
        "\n",
        "base  = AdamW(model.parameters(), lr=1e-4)\n",
        "optim = Lookahead(base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8BbyC3pMNXB",
        "outputId": "3b6ba022-95a7-405e-e719-40410f985f18"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 313/313 [08:31<00:00,  1.63s/it, loss=1.43]\n",
            "Epoch 1: 100%|██████████| 313/313 [08:28<00:00,  1.62s/it, loss=0.341]\n",
            "Epoch 2: 100%|██████████| 313/313 [08:29<00:00,  1.63s/it, loss=0.515]\n",
            "Epoch 3: 100%|██████████| 313/313 [08:30<00:00,  1.63s/it, loss=0.297]\n",
            "Epoch 4: 100%|██████████| 313/313 [08:29<00:00,  1.63s/it, loss=0.0659]\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "\n",
        "# Initialize data loader for training data\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    for batch in loop:\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                        start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "gzxXCuc5MQ1O"
      },
      "outputs": [],
      "source": [
        "# Saving the model in a local directory\n",
        "\n",
        "MODEL_DIR = \"./model\"\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    os.mkdir(MODEL_DIR)\n",
        "tokenizer.save_pretrained(MODEL_DIR)\n",
        "model.save_pretrained(MODEL_DIR)\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "MODEL_DIR = \"./model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_DIR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v8CqPTidM0ku"
      },
      "source": [
        "**Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhGph_VPMSko",
        "outputId": "04b55592-e92c-4383-f2de-dfc54c192224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score of the model based on EM: 0.5234375\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Switching model to evaluation mode\n",
        "\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "val_loader = DataLoader(valid_ds, batch_size=16)\n",
        "acc = list()\n",
        "for batch in val_loader:\n",
        "    with torch.no_grad():\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_true = batch['start_positions'].to(device)\n",
        "        end_true = batch['end_positions'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
        "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
        "        acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
        "        acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
        "        \n",
        "# Calculating average accuracy in total\n",
        "\n",
        "print(f\"Score of the model based on EM: {sum(acc)/len(acc)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFcJoqbjF2sl",
        "outputId": "66cb6654-c4ec-4f8a-e9de-205cbb207b67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score of the model: 0.378\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Switching model to evaluation mode\n",
        "\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "val_loader = DataLoader(valid_ds, batch_size=16)\n",
        "start_true_all, end_true_all = [], []\n",
        "start_pred_all, end_pred_all = [], []\n",
        "for batch in val_loader:\n",
        "    with torch.no_grad():\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_true = batch['start_positions'].to(device)\n",
        "        end_true = batch['end_positions'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
        "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
        "        start_true_all.extend(start_true.tolist())\n",
        "        end_true_all.extend(end_true.tolist())\n",
        "        start_pred_all.extend(start_pred.tolist())\n",
        "        end_pred_all.extend(end_pred.tolist())\n",
        "\n",
        "# Calculating F1 score for start and end positions\n",
        "\n",
        "start_f1 = f1_score(start_true_all, start_pred_all, average='macro')\n",
        "end_f1 = f1_score(end_true_all, end_pred_all, average='macro')\n",
        "overall_f1 = (start_f1 + end_f1) / 2\n",
        "\n",
        "print(f\"F1 score of the model: {overall_f1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "u1nMwPXxMUV5"
      },
      "outputs": [],
      "source": [
        "# Returns answer to a given a question\n",
        "def answer_to_questions(context: str, questions: list) -> list:\n",
        "    encodings = tokenizer([context]*len(questions), questions, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    encodings = encodings.to(device)\n",
        "    outputs = model(**encodings)\n",
        "    start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
        "    end_pred = torch.argmax(outputs['end_logits'], dim=1)  \n",
        "    answers = list()\n",
        "    for index, (start_idx, end_idx) in enumerate(zip(start_pred, end_pred)):\n",
        "        tokens = tokenizer.convert_ids_to_tokens(encodings['input_ids'][index][start_idx:end_idx+1])\n",
        "        answers.append(tokenizer.convert_tokens_to_string(tokens) )\n",
        "    print(\"Context:\")\n",
        "    pprint(context)\n",
        "    print()\n",
        "    for question, answer in zip(questions, answers):\n",
        "        print(f\"Q:  {question}\")\n",
        "        print(f\"A:  {answer}\")\n",
        "        print(\"-\"*60)\n",
        "    return answers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qM15P1eTM46m"
      },
      "source": [
        "**Sample answers from the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q19xOb-GM6XW",
        "outputId": "501de068-5569-460a-a20d-a04127e18cf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context:\n",
            "('The modern Olympic Games or Olympics (French: Jeux olympiques)[1][2] are '\n",
            " 'leading international sporting events featuring summer and winter sports '\n",
            " 'competitions in which thousands of athletes from around the world '\n",
            " 'participate in a variety of competitions. The Olympic Games are considered '\n",
            " \"the world's foremost sports competition with more than 200 nations \"\n",
            " 'participating.[3] The Olympic Games are normally held every four years, '\n",
            " 'alternating between the Summer and Winter Olympics every two years in the '\n",
            " 'four-year period.')\n",
            "\n",
            "Q:  How often do the Olympic games hold?\n",
            "A:  every four years,\n",
            "------------------------------------------------------------\n",
            "Q:  How many nations do participate in each Olympic?\n",
            "A:  more than 200\n",
            "------------------------------------------------------------\n",
            "Q:  what is olympics in french called as?\n",
            "A:  jeux olympiques)\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "context = \"The modern Olympic Games or Olympics (French: Jeux olympiques)[1][2] are leading international sporting events featuring summer and winter sports competitions in which thousands of athletes from around the world participate in a variety of competitions. The Olympic Games are considered the world's foremost sports competition with more than 200 nations participating.[3] The Olympic Games are normally held every four years, alternating between the Summer and Winter Olympics every two years in the four-year period.\"\n",
        "questions = [\n",
        "    \"How often do the Olympic games hold?\",\n",
        "    \"How many nations do participate in each Olympic?\",\"what is olympics in french called as?\"\n",
        "]\n",
        "\n",
        "_ = answer_to_questions(context, questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNHN_g-MM76M",
        "outputId": "0682eb67-ec78-4bd5-be33-af4310c4ce78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context:\n",
            "('Vikings is the modern name given to seafaring people primarily from '\n",
            " 'Scandinavia (present-day Denmark, Norway and Sweden), who from the late 8th '\n",
            " 'to the late 11th centuries raided, pirated, traded and settled throughout '\n",
            " 'parts of Europe. They also voyaged as far as the Mediterranean, North '\n",
            " 'Africa, the Middle East, and North America. In some of the countries they '\n",
            " 'raided and settled in, this period is popularly known as the Viking Age, and '\n",
            " 'the term \"Viking\" also commonly includes the inhabitants of the Scandinavian '\n",
            " 'homelands as a collective whole. The Vikings had a profound impact on the '\n",
            " 'Early medieval history of Scandinavia, the British Isles, France, Estonia, '\n",
            " \"and Kievan Rus'.\")\n",
            "\n",
            "Q:  When vikings started raided?\n",
            "A:  late 8th to the late 11th centuries\n",
            "------------------------------------------------------------\n",
            "Q:  who are vikings?\n",
            "A:  seafaring people\n",
            "------------------------------------------------------------\n",
            "Q:  Vikings had impact on which period?\n",
            "A:  early medieval history of scandinavia,\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "context = \"Vikings is the modern name given to seafaring people primarily from Scandinavia (present-day Denmark, Norway and Sweden), who from the late 8th to the late 11th centuries raided, pirated, traded and settled throughout parts of Europe. They also voyaged as far as the Mediterranean, North Africa, the Middle East, and North America. In some of the countries they raided and settled in, this period is popularly known as the Viking Age, and the term \\\"Viking\\\" also commonly includes the inhabitants of the Scandinavian homelands as a collective whole. The Vikings had a profound impact on the Early medieval history of Scandinavia, the British Isles, France, Estonia, and Kievan Rus'.\"\n",
        "questions = [\n",
        "    \"When vikings started raided?\",\"who are vikings?\",\"Vikings had impact on which period?\"\n",
        "]\n",
        "\n",
        "_ = answer_to_questions(context, questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3i95evNdkDo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
